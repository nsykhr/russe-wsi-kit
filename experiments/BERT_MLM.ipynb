{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Language Modeling for Word Sense Induction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have implemented a WSI algorithm inspired by the papers https://arxiv.org/pdf/1905.12598.pdf and https://www.aclweb.org/anthology/S19-2004.pdf. It is, due to time restrictions, a somewhat simplified version of those approaches. The algorithm is as follows:\n",
    "- for a set of sentences $S$ where each sentence $s_{i}$ contains some target word $w$ at least once, create a new set $S'$ by inserting a pattern that contains the [MASK] token after every occurence of $w$ (I chose the pattern __( или даже [MASK] )__ based on my preliminary research; the same pattern was found to be most effective in https://arxiv.org/pdf/1905.12598.pdf)\n",
    "- for each [MASK] token in $s'_{i}$, compute a logit vector over the BERT wordpiece vocabulary (I found it works better than post-softmax probability during my preliminary research)\n",
    "- compute the same vector in the position of each occurrence of $w$ in the original sentence $s_{i}$\n",
    "- extract top $2n$ most probable candidates for every position and concatenate them, including the logits\n",
    "- lemmatize the candidates and leave top $n$ most probable candidates for every sentence\n",
    "- represent each sentence in the set with sparse $tf-idf$ vectors constructed from these candidates\n",
    "- cluster these sparse representations with Agglomerative Clustering using cosine distance and complete linkage (I chose to infer the optimal number of clusters, one that yields the highest silhouette score, from the data)\n",
    "\n",
    "NB! I chose to use the vanilla RuBERT for this approach because it seems to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Set, Dict, Iterator, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tokenizer = RegexpTokenizer(r'[А-Яа-яЁё]+|[A-za-z]+|\\w+|[«»\\'\",.:;!?\\(\\)-–—]|[^\\w\\s]+')\n",
    "word_tokenize = re_tokenizer.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUBERT_PATH = '../../RuBERT/rubert_cased_L-12_H-768_A-12_pt'\n",
    "USE_GPU = False\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(RUBERT_PATH, do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained(RUBERT_PATH)\n",
    "if USE_GPU:\n",
    "    model = model.cuda()\n",
    "model.eval()\n",
    "device = torch.device('cuda:0' if USE_GPU else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS_ID = tokenizer.vocab[tokenizer.cls_token]\n",
    "SEP_ID = tokenizer.vocab[tokenizer.sep_token]\n",
    "MASK = tokenizer.mask_token\n",
    "\n",
    "PATTERN = ['(', 'или', 'даже', MASK, ')']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pattern(sentence: str, char_positions: List[Tuple[int, int]]) -> Tuple[List[str], List[str], Set[str]]:\n",
    "    relevant_tokens = {sentence[pos[0]:pos[1]-1] for pos in char_positions}\n",
    "    \n",
    "    tokens = word_tokenize(sentence)\n",
    "    modified_tokens = []\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        modified_tokens.append(token)\n",
    "        if token in relevant_tokens:\n",
    "            modified_tokens.extend(PATTERN)\n",
    "    \n",
    "    return tokens, modified_tokens, relevant_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Доброе утро ! Как вы поживаете ? Сегодня чудесная погода и прекрасный день ( или даже [MASK] ) , не находите ?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(apply_pattern('Доброе утро! Как вы поживаете? Сегодня чудесная погода и прекрасный день, не находите?', [(68, 73)])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.morph = MorphAnalyzer()\n",
    "        self.cache = {}\n",
    "    \n",
    "    def lemmatize(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        norm = self.morph.parse(token)[0].normal_form\n",
    "        self.cache[token] = norm\n",
    "        return norm\n",
    "\n",
    "lemmatizer = Lemmatizer()\n",
    "lemmatize = lemmatizer.lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_logits(tokens: List[str], relevant_tokens: Set[str]) -> np.ndarray:\n",
    "    input_indices, wordpiece_positions, acc_len = [], [], 1\n",
    "    for token in tokens:\n",
    "        indices = tokenizer.encode(token, add_special_tokens=False)\n",
    "        input_indices.extend(indices)\n",
    "        \n",
    "        if token in relevant_tokens:\n",
    "            wordpiece_positions.append(acc_len)\n",
    "        \n",
    "        acc_len += len(indices)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(torch.tensor([[CLS_ID] + input_indices + [SEP_ID]]).to(device))[0][0]\n",
    "    \n",
    "    return model_output[wordpiece_positions].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidates(logits: np.ndarray, n: int) -> List[str]:\n",
    "    top_candidates = [(tokenizer.convert_ids_to_tokens(int(id)), row[int(id)])\n",
    "                      for row in logits for id in row.argsort()[-2*n:]]\n",
    "    \n",
    "    filtered_candidates = set()\n",
    "    for word, _ in sorted(top_candidates, key=lambda x: x[1], reverse=True):\n",
    "        filtered_candidates.add(lemmatize(word))\n",
    "        if len(filtered_candidates) == n:\n",
    "            break\n",
    "    \n",
    "    return list(filtered_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(sentence: str, char_positions: List[Tuple[int, int]], n: int) -> List[str]:\n",
    "    _, modified_tokens, _ = apply_pattern(sentence, char_positions)\n",
    "    logits = get_token_logits(modified_tokens, {MASK,})\n",
    "    \n",
    "    return extract_candidates(logits, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['прекрасный',\n",
       " 'утро',\n",
       " 'отличный',\n",
       " 'ночь',\n",
       " 'вечер',\n",
       " 'праздник',\n",
       " 'день',\n",
       " 'воскресение',\n",
       " 'хороший',\n",
       " 'выходной']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates('- Доброе утро! Как вы поживаете? Сегодня чудесная погода и прекрасный день, не находите? - Да, сегодня отличный день.',\n",
    "               [(70, 75), (112, 117)], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = lambda x: x\n",
    "\n",
    "def create_tfidf_matrix(data: List[Tuple[str, List[Tuple[int, int]], str, int]], n: int = 50) -> np.ndarray:\n",
    "    candidate_set = [get_candidates(sent, pos, n) for sent, pos, _, _ in data]\n",
    "    \n",
    "    tfidf = TfidfVectorizer(preprocessor=dummy, tokenizer=dummy, lowercase=False)\n",
    "    return tfidf.fit_transform(candidate_set).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_id</th>\n",
       "      <th>word</th>\n",
       "      <th>gold_sense_id</th>\n",
       "      <th>predict_sense_id</th>\n",
       "      <th>positions</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>дар</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(18, 22)]</td>\n",
       "      <td>Отвергнуть щедрый дар</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>дар</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(21, 28)]</td>\n",
       "      <td>покупать преданность дарами и наградами</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>дар</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(19, 23)]</td>\n",
       "      <td>Вот яд – последний дар моей Изоры</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>дар</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(81, 87)]</td>\n",
       "      <td>Основная функция корильных песен – повеселить ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>дар</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(151, 157)]</td>\n",
       "      <td>Но недели две спустя (Алевтина его когда-то об...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2068</td>\n",
       "      <td>2069</td>\n",
       "      <td>зонт</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(85, 91)]</td>\n",
       "      <td>Такая погода легко переживается весной, а вот ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2069</td>\n",
       "      <td>2070</td>\n",
       "      <td>зонт</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(8, 13)]</td>\n",
       "      <td>Пляжный зонт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>2071</td>\n",
       "      <td>зонт</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(18, 25)]</td>\n",
       "      <td>сидеть в кафе под зонтом</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2071</td>\n",
       "      <td>2072</td>\n",
       "      <td>зонт</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(21, 29)]</td>\n",
       "      <td>Cтолики под широкими зонтами, несколько привин...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2072</td>\n",
       "      <td>2073</td>\n",
       "      <td>зонт</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(62, 70)]</td>\n",
       "      <td>Я выскочила из Исторического музея в летнее ка...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2040 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      context_id  word gold_sense_id  predict_sense_id     positions  \\\n",
       "0              1   дар             1               NaN    [(18, 22)]   \n",
       "1              2   дар             1               NaN    [(21, 28)]   \n",
       "2              3   дар             1               NaN    [(19, 23)]   \n",
       "3              4   дар             1               NaN    [(81, 87)]   \n",
       "4              5   дар             1               NaN  [(151, 157)]   \n",
       "...          ...   ...           ...               ...           ...   \n",
       "2068        2069  зонт             1               NaN    [(85, 91)]   \n",
       "2069        2070  зонт             2               NaN     [(8, 13)]   \n",
       "2070        2071  зонт             2               NaN    [(18, 25)]   \n",
       "2071        2072  зонт             2               NaN    [(21, 29)]   \n",
       "2072        2073  зонт             2               NaN    [(62, 70)]   \n",
       "\n",
       "                                                context  \n",
       "0                                 Отвергнуть щедрый дар  \n",
       "1               покупать преданность дарами и наградами  \n",
       "2                     Вот яд – последний дар моей Изоры  \n",
       "3     Основная функция корильных песен – повеселить ...  \n",
       "4     Но недели две спустя (Алевтина его когда-то об...  \n",
       "...                                                 ...  \n",
       "2068  Такая погода легко переживается весной, а вот ...  \n",
       "2069                                       Пляжный зонт  \n",
       "2070                           сидеть в кафе под зонтом  \n",
       "2071  Cтолики под широкими зонтами, несколько привин...  \n",
       "2072  Я выскочила из Исторического музея в летнее ка...  \n",
       "\n",
       "[2040 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/nsykhr/russe-wsi-kit/master/data/main/active-dict/train.csv', sep='\\t')\n",
    "data.dropna(subset=['positions'], inplace=True) # let's drop the rows where the relevant token is not present in the context\n",
    "data.positions = data.positions.apply(lambda x: [tuple(map(int, pos.split('-'))) for pos in x.split(',')])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CLUSTERS = 8\n",
    "\n",
    "def find_optimal_clustering(estimator_class, X, kwargs) -> int:\n",
    "    best_score, best_clustering = -1, None\n",
    "\n",
    "    for n_clusters in range(2, min(MAX_CLUSTERS, len(X) - 1)):\n",
    "        kwargs['n_clusters'] = n_clusters\n",
    "        estimator = estimator_class(**kwargs)\n",
    "        preds = estimator.fit_predict(X)\n",
    "        score = silhouette_score(X, preds)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_clustering = preds\n",
    "\n",
    "    return best_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_evaluate(data: pd.DataFrame):\n",
    "    data_by_words = {key: [] for key in data.word.unique()}\n",
    "    for i, row in data.iterrows():\n",
    "        data_by_words[row.word].append((row.context, row.positions, row.gold_sense_id, i))\n",
    "    \n",
    "    aggl = AgglomerativeClustering\n",
    "    kwargs = {'affinity': 'cosine', 'linkage': 'complete'}\n",
    "\n",
    "    print('word\\tARI\\tcount')\n",
    "\n",
    "    score = 0\n",
    "    size = sum(len(word_data) for word_data in data_by_words.values())\n",
    "\n",
    "    for word, word_data in data_by_words.items():\n",
    "        labels = [id for _, _, id, _ in word_data]\n",
    "        tfidf_matrix = create_tfidf_matrix(word_data, n=50)\n",
    "        preds = find_optimal_clustering(aggl, tfidf_matrix, kwargs)\n",
    "\n",
    "        ari = adjusted_rand_score(labels, preds)\n",
    "        print(f'{word}\\t{ari}\\t{len(word_data)}')\n",
    "\n",
    "        score += ari * len(word_data) / size\n",
    "\n",
    "    print(f'\\noverall\\t{score}\\t{size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\tARI\tcount\n",
      "дар\t0.07864534830340632\t36\n",
      "двигатель\t0.47093023255813954\t14\n",
      "двойник\t0.02067464635473339\t25\n",
      "дворец\t0.691699604743083\t13\n",
      "девятка\t0.12519370501739038\t47\n",
      "дедушка\t0.55\t9\n",
      "дежурная\t-0.11594202898550726\t12\n",
      "дежурный\t0.07142857142857137\t13\n",
      "декабрист\t0.07903549899531145\t11\n",
      "декрет\t0.664819944598338\t12\n",
      "дело\t0.06755265427538187\t129\n",
      "демобилизация\t0.7144581221337195\t14\n",
      "демократ\t0.20872250096487846\t18\n",
      "демонстрация\t0.3424030650253645\t37\n",
      "дерево\t0.38636363636363635\t21\n",
      "держава\t0.050131926121372065\t15\n",
      "дерзость\t0.08514621787175156\t37\n",
      "десятка\t-0.008636525998877391\t36\n",
      "десяток\t-0.007067137809187302\t20\n",
      "деятель\t0.7130730050933786\t14\n",
      "диалог\t0.22980251346499103\t14\n",
      "диаметр\t0.016528925619834742\t18\n",
      "диплом\t0.14788097385031554\t25\n",
      "директор\t-0.07843137254901954\t11\n",
      "диск\t0.19577812912780693\t62\n",
      "дичь\t0.04198895027624308\t18\n",
      "длина\t0.021709633649932215\t21\n",
      "доброволец\t0.18331805682859767\t12\n",
      "добыча\t0.37547647195306544\t35\n",
      "доказательство\t0.03937350445943006\t24\n",
      "доктор\t1.0\t17\n",
      "долгота\t0.09684210526315788\t13\n",
      "доля\t0.3307596452219806\t45\n",
      "дом\t0.205963725791577\t36\n",
      "дорога\t0.1578105699099441\t47\n",
      "достижение\t0.46640316205533594\t22\n",
      "древесина\t0.09800520381613184\t16\n",
      "дупло\t0.12353923205342239\t15\n",
      "дура\t0.18207816968541465\t12\n",
      "дух\t0.08212545955507954\t75\n",
      "дым\t0.2959701997968168\t28\n",
      "дымка\t0.07855459544383347\t18\n",
      "дыхание\t0.14074964791533123\t55\n",
      "дьявол\t0.09617834394904455\t22\n",
      "евро\t-0.15384615384615385\t8\n",
      "езда\t0.04072831816003834\t14\n",
      "жаворонок\t-0.031250000000000014\t10\n",
      "жало\t0.0931899641577061\t11\n",
      "жертва\t0.24177968858278295\t37\n",
      "жестокость\t-0.08982035928143713\t14\n",
      "жидкость\t-0.05217391304347827\t12\n",
      "жила\t-0.07441860465116279\t7\n",
      "жилец\t-0.06224066390041494\t16\n",
      "жир\t0.17670682730923698\t15\n",
      "жребий\t0.1342281879194631\t15\n",
      "заведение\t0.3220338983050847\t11\n",
      "завещание\t0.31666666666666665\t16\n",
      "зависимость\t0.2889996167113837\t21\n",
      "заголовок\t0.3628103196495213\t22\n",
      "заготовка\t0.5530666503682425\t26\n",
      "задание\t0.12487742485610744\t33\n",
      "задача\t0.1226241569589209\t36\n",
      "задержка\t0.06631895220741328\t55\n",
      "зажигалка\t0.19753086419753083\t13\n",
      "закон\t0.08537490720118783\t56\n",
      "закрытие\t0.03465574273258997\t37\n",
      "заложник\t0.1551362683438155\t13\n",
      "замена\t0.42127659574468085\t16\n",
      "западня\t-0.01583113456464384\t11\n",
      "запятая\t-0.08254430687059969\t14\n",
      "застой\t0.21946169772256727\t13\n",
      "затея\t0.4005449591280655\t12\n",
      "затишье\t-0.031775700934579425\t16\n",
      "затмение\t-0.03587443946188337\t12\n",
      "затруднение\t0.24040066777963273\t15\n",
      "захоронение\t0.2313860252004582\t22\n",
      "звезда\t0.35784851811196483\t40\n",
      "звон\t0.12077294685990342\t14\n",
      "зеркало\t0.11116257526632703\t20\n",
      "зло\t0.00017370912407176038\t23\n",
      "злоупотребление\t0.5023562676720075\t12\n",
      "знак\t0.1581105027149914\t55\n",
      "знамя\t0.21250000000000002\t14\n",
      "значение\t0.245639090074467\t30\n",
      "зонт\t0.1818181818181818\t9\n",
      "\n",
      "overall\t0.17464853536732722\t2040\n"
     ]
    }
   ],
   "source": [
    "cluster_and_evaluate(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_id</th>\n",
       "      <th>word</th>\n",
       "      <th>gold_sense_id</th>\n",
       "      <th>predict_sense_id</th>\n",
       "      <th>positions</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2074</td>\n",
       "      <td>давление</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(0, 9)]</td>\n",
       "      <td>Давление пара создается движением поршня в цил...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2075</td>\n",
       "      <td>давление</td>\n",
       "      <td>2.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(13, 22)]</td>\n",
       "      <td>«У тебя что, давление поднялось?» Я сказал, чт...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2076</td>\n",
       "      <td>давление</td>\n",
       "      <td>2.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(56, 65)]</td>\n",
       "      <td>Я жалуюсь Никоновичу наконец на головокружение...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2077</td>\n",
       "      <td>давление</td>\n",
       "      <td>2.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(0, 9)]</td>\n",
       "      <td>Давление в котле не менялось</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2078</td>\n",
       "      <td>давление</td>\n",
       "      <td>2.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(25, 34)]</td>\n",
       "      <td>Он каждые два часа мерил давление и сахар в крови</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3724</td>\n",
       "      <td>5798</td>\n",
       "      <td>зуд</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(43, 47)]</td>\n",
       "      <td>Многих американцев одолевает романтический зуд...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3725</td>\n",
       "      <td>5799</td>\n",
       "      <td>зуд</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(23, 27)]</td>\n",
       "      <td>Если на нее не находил зуд рассказывания истор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3726</td>\n",
       "      <td>5800</td>\n",
       "      <td>зуд</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(27, 33)]</td>\n",
       "      <td>С раздражающей завистью, с зудом неудовлетворе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3727</td>\n",
       "      <td>5801</td>\n",
       "      <td>зуд</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(12, 16)]</td>\n",
       "      <td>Нестерпимый зуд любопытства</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3728</td>\n",
       "      <td>5802</td>\n",
       "      <td>зуд</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(17, 21)]</td>\n",
       "      <td>В ноге был такой зуд, что Матвееву хотелось сн...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3685 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      context_id      word gold_sense_id  predict_sense_id   positions  \\\n",
       "0           2074  давление             1               NaN    [(0, 9)]   \n",
       "1           2075  давление           2.2               NaN  [(13, 22)]   \n",
       "2           2076  давление           2.2               NaN  [(56, 65)]   \n",
       "3           2077  давление           2.1               NaN    [(0, 9)]   \n",
       "4           2078  давление           2.2               NaN  [(25, 34)]   \n",
       "...          ...       ...           ...               ...         ...   \n",
       "3724        5798       зуд             2               NaN  [(43, 47)]   \n",
       "3725        5799       зуд             2               NaN  [(23, 27)]   \n",
       "3726        5800       зуд             2               NaN  [(27, 33)]   \n",
       "3727        5801       зуд             2               NaN  [(12, 16)]   \n",
       "3728        5802       зуд             1               NaN  [(17, 21)]   \n",
       "\n",
       "                                                context  \n",
       "0     Давление пара создается движением поршня в цил...  \n",
       "1     «У тебя что, давление поднялось?» Я сказал, чт...  \n",
       "2     Я жалуюсь Никоновичу наконец на головокружение...  \n",
       "3                          Давление в котле не менялось  \n",
       "4     Он каждые два часа мерил давление и сахар в крови  \n",
       "...                                                 ...  \n",
       "3724  Многих американцев одолевает романтический зуд...  \n",
       "3725  Если на нее не находил зуд рассказывания истор...  \n",
       "3726  С раздражающей завистью, с зудом неудовлетворе...  \n",
       "3727                        Нестерпимый зуд любопытства  \n",
       "3728  В ноге был такой зуд, что Матвееву хотелось сн...  \n",
       "\n",
       "[3685 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/nsykhr/russe-wsi-kit/master/data/main/active-dict/test-solution.csv', sep='\\t')\n",
    "data.dropna(subset=['positions'], inplace=True) # let's drop the rows where the relevant token is not present in the context\n",
    "data.positions = data.positions.apply(lambda x: [tuple(map(int, pos.split('-'))) for pos in x.split(',')])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_write_to_pandas(data: pd.DataFrame):\n",
    "    data_by_words = {key: [] for key in data.word.unique()}\n",
    "    for i, row in data.iterrows():\n",
    "        data_by_words[row.word].append((row.context, row.positions, row.gold_sense_id, i))\n",
    "    \n",
    "    aggl = AgglomerativeClustering\n",
    "    kwargs = {'affinity': 'cosine', 'linkage': 'complete'}\n",
    "\n",
    "    for word, word_data in tqdm(data_by_words.items()):\n",
    "        tfidf_matrix = create_tfidf_matrix(word_data, n=50)\n",
    "        preds = find_optimal_clustering(aggl, tfidf_matrix, kwargs)\n",
    "\n",
    "        for pred, row in zip(preds, word_data):\n",
    "            data.predict_sense_id.loc[row[-1]] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [21:03<00:00,  7.52s/it]\n"
     ]
    }
   ],
   "source": [
    "cluster_and_write_to_pandas(data)\n",
    "data.to_csv('../data/main/active-dict/result_mlm_aggl.csv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm achieves __0.190087__ ARI on the test data. Not quite as good as the other approach I tried, but still significantly better than the baseline. Let's see how it does on the RuTenTen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_id</th>\n",
       "      <th>word</th>\n",
       "      <th>gold_sense_id</th>\n",
       "      <th>predict_sense_id</th>\n",
       "      <th>positions</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>альбом</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(88, 96)]</td>\n",
       "      <td>достаточно лишь колесиком мышки крутить вниз. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>альбом</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(85, 93)]</td>\n",
       "      <td>выступал в составе команды с таким названием, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>альбом</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(81, 89)]</td>\n",
       "      <td>. Работает так себе, поскольку функция заточен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>альбом</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(84, 91)]</td>\n",
       "      <td>одержала победу в двух из пяти номинаций: 'Луч...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>альбом</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(83, 90)]</td>\n",
       "      <td>встречи с Божественным. Вы испытаете ни с чем ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3666</td>\n",
       "      <td>3667</td>\n",
       "      <td>группа</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(102, 109)]</td>\n",
       "      <td>напротив, цветет пуще прежнего. География расш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3667</td>\n",
       "      <td>3668</td>\n",
       "      <td>группа</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(93, 100)]</td>\n",
       "      <td>синтетической работе, терпение и упорство, жел...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3668</td>\n",
       "      <td>3669</td>\n",
       "      <td>группа</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(20, 27)]</td>\n",
       "      <td>Маркетинг процедуры.Группа компаний Кивеннапа ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3669</td>\n",
       "      <td>3670</td>\n",
       "      <td>группа</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(100, 107)]</td>\n",
       "      <td>International» признались миллионам слушателей...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3670</td>\n",
       "      <td>3671</td>\n",
       "      <td>группа</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(103, 110)]</td>\n",
       "      <td>Абрамовым, Гарик Мартиросян с женой Жанной, Ти...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3671 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      context_id    word  gold_sense_id  predict_sense_id     positions  \\\n",
       "0              1  альбом              2               NaN    [(88, 96)]   \n",
       "1              2  альбом              3               NaN    [(85, 93)]   \n",
       "2              3  альбом              2               NaN    [(81, 89)]   \n",
       "3              4  альбом              3               NaN    [(84, 91)]   \n",
       "4              5  альбом              3               NaN    [(83, 90)]   \n",
       "...          ...     ...            ...               ...           ...   \n",
       "3666        3667  группа              4               NaN  [(102, 109)]   \n",
       "3667        3668  группа              4               NaN   [(93, 100)]   \n",
       "3668        3669  группа              4               NaN    [(20, 27)]   \n",
       "3669        3670  группа              4               NaN  [(100, 107)]   \n",
       "3670        3671  группа              4               NaN  [(103, 110)]   \n",
       "\n",
       "                                                context  \n",
       "0     достаточно лишь колесиком мышки крутить вниз. ...  \n",
       "1     выступал в составе команды с таким названием, ...  \n",
       "2     . Работает так себе, поскольку функция заточен...  \n",
       "3     одержала победу в двух из пяти номинаций: 'Луч...  \n",
       "4     встречи с Божественным. Вы испытаете ни с чем ...  \n",
       "...                                                 ...  \n",
       "3666  напротив, цветет пуще прежнего. География расш...  \n",
       "3667  синтетической работе, терпение и упорство, жел...  \n",
       "3668  Маркетинг процедуры.Группа компаний Кивеннапа ...  \n",
       "3669  International» признались миллионам слушателей...  \n",
       "3670  Абрамовым, Гарик Мартиросян с женой Жанной, Ти...  \n",
       "\n",
       "[3671 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/nsykhr/russe-wsi-kit/master/data/additional/active-rutenten/train.csv', sep='\\t')\n",
    "data.dropna(subset=['positions'], inplace=True) # let's drop the rows where the relevant token is not present in the context\n",
    "data.positions = data.positions.apply(lambda x: [tuple(map(int, pos.split('-'))) for pos in x.split(',')])\n",
    "data.positions = data.positions.apply(lambda x: [(a, b+2) for a, b in x])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.context.loc[494] = '15, 000 на всех членов семьи.Вот тогда и законы будут человечными Анатомия и физиология человека'\n",
    "data.positions.loc[494] = [(66, 75)]\n",
    "\n",
    "data.context.loc[1202] = 'выше чем 1 метр или до 12 лет-8 евро, дети ниже чем 1 метр –вход бесплатный Билеты: взрослые-16 евро, дети выше чем 1 метр или старше 10 лет-12 евро, дети до 10 лет'\n",
    "data.positions.loc[1202] = [(76, 83)]\n",
    "\n",
    "data.context.loc[1381] = 'крепление Крепление на стену по стандарту VESA 100мм Блок питания внешний'\n",
    "data.positions.loc[1381] = [(53, 58)]\n",
    "\n",
    "data.context.loc[1398] = 'Пористые заполнители Блоки оконные'\n",
    "data.positions.loc[1398] = [(21, 27)]\n",
    "\n",
    "data.context.loc[1432] = 'СКАТ БЛОКИ ПИТАНИЯ БПУ-24-0,5; БПУ-24-0,7; БПУ-12 -1,5; БПС -12 -0,7; БП-TV1; БП-TV3 БЛОК ПИТАНИЯ СЕТЕВОЙ БПС СИСТЕМА ДИСТАНЦИОННОГО... БПС -12 . Блок питания с симисторами'\n",
    "data.positions.loc[1432] = [(85, 90)]\n",
    "\n",
    "data.context.loc[1514] = 'Библиографические ресурсы и каталоги Блок библиографических ресурсов глобальных сетей обширен и разнообразен. Его главной'\n",
    "data.positions.loc[1514] = [(37, 42)]\n",
    "\n",
    "data.context.loc[2019] = 'Выпускаемая продукция Вешалка детская'\n",
    "data.positions.loc[2019] = [(22, 30)]\n",
    "\n",
    "data.context.loc[2566] = 'Электропроводка для подключения светодиодных знаков в задней части прицепа вилка/розетка) для подключения электрооборудования прицепа к электросети автомобиля'\n",
    "data.positions.loc[2566] = [(75, 81)]\n",
    "\n",
    "data.context.loc[2811] = 'Волги только левым расположением запасного колеса. Оно так же прикручено винтом. горизонтальным торсионам и удерживалась ими в открытом положении. Причем оригинальной'\n",
    "data.positions.loc[2811] = [(73, 80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [36:31<00:00, 109.60s/it] \n"
     ]
    }
   ],
   "source": [
    "cluster_and_write_to_pandas(data)\n",
    "data.to_csv('../data/additional/active-rutenten/result_mlm_aggl.csv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The achieved ARI is __0.188276__. I wasn't able to beat the baseline on this dataset yet, but it is definitely possible if one invests some more time into refining every component of this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
